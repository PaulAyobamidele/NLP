{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulAyobamidele/NLP/blob/main/dataprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basics in NLP 1**"
      ],
      "metadata": {
        "id": "oI2w2eIxNuUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "Ez1g5oD2Qbfp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Acquisition**"
      ],
      "metadata": {
        "id": "wC1WGGJ_N3Y4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azyTGbtbf29P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "cbcaa700-8983-4334-c020-d6cf2ea53dbf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import requests\n",
        "import re\n",
        "\n",
        "def fetch_and_save_wiki_text(title):\n",
        "    response = requests.get(\n",
        "        \"https://en.wikipedia.org/w/api.php\",\n",
        "        params={\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"titles\": title,\n",
        "            \"prop\": \"extracts\",\n",
        "            \"explaintext\": True,\n",
        "        },\n",
        "    ).json()\n",
        "\n",
        "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
        "    wiki_text = page[\"extract\"]\n",
        "\n",
        "    return wiki_text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "city = 'Tokyo'\n",
        "info = fetch_and_save_wiki_text(city)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "a4vgbI0bP1Wl",
        "outputId": "147fd30a-8738-4207-af9f-80688bdb9dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(info)"
      ],
      "metadata": {
        "id": "HhnbH013QBHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Cleaning using Regex**"
      ],
      "metadata": {
        "id": "PC85-g3AR8o7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove special characters except \".\"\n",
        "    #text = re.sub(r'[^A-Za-z0-9\\s.\\(\\)\\[\\]\\{\\}]+', '', text)\n",
        "    text = re.sub(r'[^A-Za-z\\s.]+','',text)\n",
        "    # remove HTML TAG\n",
        "    html = re.compile('[<,#*?>]')\n",
        "    text = html.sub(r'',text)\n",
        "    # Remove urls:\n",
        "    url = re.compile('https?://\\S+|www\\.S+')\n",
        "    text = url.sub(r'',text)\n",
        "    # Remove email id:\n",
        "    email = re.compile('[A-Za-z0-2]+@[\\w]+.[\\w]+')\n",
        "    text = email.sub(r'',text)\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ],
      "metadata": {
        "id": "jMqJt06fgOOE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ec9a9650-4ef8-4bb1-dc40-67ca4f96e327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean = clean_text(info)\n",
        "print(clean)"
      ],
      "metadata": {
        "id": "LxvepkCHgWpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization**"
      ],
      "metadata": {
        "id": "DEKYMWnUmRwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenization using NLTK**"
      ],
      "metadata": {
        "id": "zQa_QO88SD3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK is Natural Language Tool Kit. It is used to build python programming. It helps to work with human languages data. It gives a very easy user interface. It supports classification, steaming, tagging, etc."
      ],
      "metadata": {
        "id": "9AUvlPZBSM05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GRZieNJ4SYLG",
        "outputId": "013fc4a3-a453-4ac8-d787-0f068cd7f641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next code will take quite some time due to the massive amount of tokenizers, chunkers, other algorithms, and all of the corpora to be downloaded."
      ],
      "metadata": {
        "id": "v3DbPkvSTP8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "ZHleqoJZTTFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Sentence Tokenization**"
      ],
      "metadata": {
        "id": "UxdH4kqWSbvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sentences = sent_tokenize(clean)\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  print(\"Sentence \" + str(i) + \" : \")\n",
        "  print(sentences[i])"
      ],
      "metadata": {
        "id": "O793__9cQQFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Word Tokenization**"
      ],
      "metadata": {
        "id": "nRuIAvrhUL7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "words = word_tokenize(clean)\n",
        "\n",
        "for i in range(len(words)):\n",
        "  print(words[i])"
      ],
      "metadata": {
        "id": "ymXnuJrJUL7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These tokenizers work by separating the words using punctuation and spaces, and it doesn’t discard the punctuation, allowing a user to decide what to do with the punctuations at the time of pre-processing."
      ],
      "metadata": {
        "id": "U8fJ3eV4U0OL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Regexp Tokenization**"
      ],
      "metadata": {
        "id": "a2xrfTr5XQ46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[^ ][\\w']*\")\n",
        "regwords = tokenizer.tokenize(clean)\n",
        "print(regwords)"
      ],
      "metadata": {
        "id": "6O0OUYvJUhTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lemmatization**"
      ],
      "metadata": {
        "id": "31Mm2GWfXnxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Wordnet Lemmatizer**\n",
        "\n"
      ],
      "metadata": {
        "id": "W_QGX7RDXvIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[WordNet®](https://wordnet.princeton.edu/) is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations."
      ],
      "metadata": {
        "id": "6yU1al2W7M3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wordnet is a publicly available lexical database of over 200 languages that provides semantic relationships between its words. It is one of the earliest and most commonly used lemmatizer technique.  \n",
        "\n",
        "*   It is present in the nltk library in python.\n",
        "*   Wordnet links words into semantic relations. ( eg. synonyms )\n",
        "*   It groups synonyms in the form of `synsets` (a group of data elements that are semantically equivalent.)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "How to use:  \n",
        "\n",
        "Download Wordnet from nltk\n",
        "*   import nltk\n",
        "*   nltk.download(‘wordnet’)\n",
        "*   nltk.download(‘averaged_perceptron_tagger’)"
      ],
      "metadata": {
        "id": "MUA5VzQBX7Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create WordNetLemmatizer object\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# single word lemmatization\n",
        "for word in words[:100]:\n",
        "    print(word + \" ---> \" + wnl.lemmatize(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "DKvm5DktWvZK",
        "outputId": "6d2dd7c2-edf4-4128-c213-1ec4da102136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokyo Japanese Tky toko officially the Tokyo Metropolis Tkyto is the capital of Japan and the most populous city in the world with a population of over million residents as of . ---> Tokyo Japanese Tky toko officially the Tokyo Metropolis Tkyto is the capital of Japan and the most populous city in the world with a population of over million residents as of .\n",
            "The Tokyo metropolitan area which includes Tokyo and nearby prefectures is the worlds mostpopulous metropolitan area with . ---> The Tokyo metropolitan area which includes Tokyo and nearby prefectures is the worlds mostpopulous metropolitan area with .\n",
            "million residents as of and is the secondlargest metropolitan economy in the world after New York City with a gross metropolitan product estimated at US trillion. ---> million residents as of and is the secondlargest metropolitan economy in the world after New York City with a gross metropolitan product estimated at US trillion.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Wordnet Lemmatizer (with POS tag)**"
      ],
      "metadata": {
        "id": "Yo1a4CCrfXi0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above approach, we observed that Wordnet results were not up to the mark. Words like ‘located’, ‘includes’ etc remained the same after lemmatization. This is because these words are treated as a noun in the given sentence rather than a verb. To overcome come this, we use POS (Part of Speech) tags.\n",
        "\n",
        "We add a tag with a particular word defining its type (verb, noun, adjective etc).\n"
      ],
      "metadata": {
        "id": "jqGuwQqBfrFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WORDNET LEMMATIZER (with appropriate pos tags)\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "4nNLr0T8hZuI",
        "outputId": "0a17bf24-2817-49a1-91b3-ac3f9dabe8a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define function to lemmatize each word with its POS tag\n",
        "\n",
        "# POS_TAGGER_FUNCTION\n",
        "def pos_tagger(nltk_tag):\n",
        "\tif nltk_tag.startswith('J'):\n",
        "\t\treturn wordnet.ADJ\n",
        "\telif nltk_tag.startswith('V'):\n",
        "\t\treturn wordnet.VERB\n",
        "\telif nltk_tag.startswith('N'):\n",
        "\t\treturn wordnet.NOUN\n",
        "\telif nltk_tag.startswith('R'):\n",
        "\t\treturn wordnet.ADV\n",
        "\telse:\n",
        "\t\treturn None\n",
        "\n",
        "sentence = sentences[4]\n",
        "print(\"Original sentence: \")\n",
        "print(sentence)\n",
        "print(\"---------------------\")\n",
        "\n",
        "# tokenize the sentence and find the POS tag for each token\n",
        "pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "print(\"POS results:\")\n",
        "print(pos_tagged)\n",
        "print(\"---------------------\")\n",
        "\n",
        "# As you may have noticed, the above pos tags are a little confusing.\n",
        "\n",
        "# we use our own pos_tagger function to make things simpler to understand.\n",
        "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
        "print(\"Our POS results:\")\n",
        "print(wordnet_tagged)\n",
        "print(\"---------------------\")\n",
        "\n",
        "lemmatized_sentence = []\n",
        "for word, tag in wordnet_tagged:\n",
        "\tif tag is None:\n",
        "\t\t# if there is no available tag, append the token as is\n",
        "\t\tlemmatized_sentence.append(word)\n",
        "\telse:\n",
        "\t\t# else use the tag to lemmatize the token\n",
        "\t\tlemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
        "\n",
        "print(\"Final tokenization results:\")\n",
        "print(lemmatized_sentence)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "JADyxwXgYs7Y",
        "outputId": "f5a6304e-22b8-48ac-acaf-589ddd602416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: \n",
            "Tokyo serves as Japans economic center and the seat of both the Japanese government and the Emperor of Japan.\n",
            "---------------------\n",
            "POS results:\n",
            "[('Tokyo', 'NNP'), ('serves', 'VBZ'), ('as', 'IN'), ('Japans', 'NNPS'), ('economic', 'JJ'), ('center', 'NN'), ('and', 'CC'), ('the', 'DT'), ('seat', 'NN'), ('of', 'IN'), ('both', 'DT'), ('the', 'DT'), ('Japanese', 'JJ'), ('government', 'NN'), ('and', 'CC'), ('the', 'DT'), ('Emperor', 'NNP'), ('of', 'IN'), ('Japan', 'NNP'), ('.', '.')]\n",
            "---------------------\n",
            "Our POS results:\n",
            "[('Tokyo', 'n'), ('serves', 'v'), ('as', None), ('Japans', 'n'), ('economic', 'a'), ('center', 'n'), ('and', None), ('the', None), ('seat', 'n'), ('of', None), ('both', None), ('the', None), ('Japanese', 'a'), ('government', 'n'), ('and', None), ('the', None), ('Emperor', 'n'), ('of', None), ('Japan', 'n'), ('.', None)]\n",
            "---------------------\n",
            "Final tokenization results:\n",
            "Tokyo serve as Japans economic center and the seat of both the Japanese government and the Emperor of Japan .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. TextBlob Lemmatizer**"
      ],
      "metadata": {
        "id": "osJ68xANiadv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextBlob is a python library used for processing textual data. It provides a simple API to access its methods and perform basic NLP tasks.\n",
        "\n",
        "First, download TextBlob package:\n"
      ],
      "metadata": {
        "id": "88amhoLFipAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "id": "hq820TkygkH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob, Word\n",
        "\n",
        "\n",
        "sentence = sentences[15]\n",
        "print(\"Original sentence: \")\n",
        "print(sentence)\n",
        "print(\"---------------------\")\n",
        "\n",
        "s = TextBlob(sentence)\n",
        "lemmatized_sentence = \" \".join([w.lemmatize() for w in s.words])\n",
        "\n",
        "print(\"Final tokenization results:\")\n",
        "print(lemmatized_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "Wrk_ICjKix11",
        "outputId": "a78d5381-bbd1-43b1-e9d9-6accc574034d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: \n",
            "As of the city is home to of the worlds largest companies listed in the annual Fortune Global .In Tokyo ranked fourth on the Global Financial Centres Index behind New York City London and Shanghai.\n",
            "---------------------\n",
            "Final tokenization results:\n",
            "As of the city is home to of the world largest company listed in the annual Fortune Global In Tokyo ranked fourth on the Global Financial Centres Index behind New York City London and Shanghai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. TextBlob (with POS tag)**"
      ],
      "metadata": {
        "id": "qH5wdT6LjrNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as in Wordnet approach without using appropriate POS tags, we observe the same limitations in this approach as well. So, we use one of the more powerful aspects of the TextBlob module the ‘Part of Speech’ tagging to overcome this problem."
      ],
      "metadata": {
        "id": "mkCzvc6rjtbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Define function to lemmatize each word with its POS tag\n",
        "\n",
        "# POS_TAGGER_FUNCTION\n",
        "def pos_tagger_blob(sentence):\n",
        "\tsent = TextBlob(sentence)\n",
        "\ttag_dict = {\"J\": 'a', \"N\": 'n', \"V\": 'v', \"R\": 'r'}\n",
        "\twords_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]\n",
        "\tlemma_list = [wd.lemmatize(tag) for wd, tag in words_tags]\n",
        "\treturn lemma_list\n",
        "\n",
        "# Lemmatize\n",
        "sentence = sentences[15]\n",
        "print(\"Original sentence: \")\n",
        "print(sentence)\n",
        "print(\"---------------------\")\n",
        "\n",
        "lemma_list = pos_tagger_blob(sentence)\n",
        "lemmatized_sentence = \" \".join(lemma_list)\n",
        "print(\"Final tokenization results:\")\n",
        "print(lemmatized_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "N52eATIDi6Ii",
        "outputId": "85bf164e-699e-4844-e656-f8d680499ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: \n",
            "As of the city is home to of the worlds largest companies listed in the annual Fortune Global .In Tokyo ranked fourth on the Global Financial Centres Index behind New York City London and Shanghai.\n",
            "---------------------\n",
            "Final tokenization results:\n",
            "As of the city be home to of the world large company list in the annual Fortune Global .In Tokyo rank fourth on the Global Financial Centres Index behind New York City London and Shanghai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. spaCy**"
      ],
      "metadata": {
        "id": "AhalfvvKkjUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy is an open-source python library that parses and “understands” large volumes of text. Separate models are available that cater to specific languages (English, French, German, etc.)."
      ],
      "metadata": {
        "id": "jjA-jy12km3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-noaU_sLkPnh",
        "outputId": "fb55e8ff-5d8c-40eb-f11c-73584ef32b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "rEg99ODBk-Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Doc object\n",
        "doc = nlp(u'Japanese author Haruki Murakami has based some of his novels in Tokyo including Norwegian Wood and David Mitchells first two novels numberdream and Ghostwritten featured the city.')\n",
        "\n",
        "# Create list of tokens from given string\n",
        "tokens = []\n",
        "for token in doc:\n",
        "\ttokens.append(token)\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "print(lemmatized_sentence)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mNFQZ5BBkwZC",
        "outputId": "be33c3e9-f580-447c-cabc-8d4c68f75a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Japanese, author, Haruki, Murakami, has, based, some, of, his, novels, in, Tokyo, including, Norwegian, Wood, and, David, Mitchells, first, two, novels, numberdream, and, Ghostwritten, featured, the, city, .]\n",
            "japanese author Haruki Murakami have base some of his novel in Tokyo include Norwegian Wood and David Mitchells first two novel numberdream and Ghostwritten feature the city .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "M6XEsQPbl6bO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stemmming**"
      ],
      "metadata": {
        "id": "3lp4YA7bmo__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python NLTK contains a variety of stemming algorithms, including several types. Let’s examine them down below."
      ],
      "metadata": {
        "id": "JOU71Lzjm6eb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Porter’s Stemmer**"
      ],
      "metadata": {
        "id": "tXOMPEt_m9ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Create a Porter Stemmer instance\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# Example words for stemming\n",
        "#originals = [\"running\", \"better\", \"mice\", \"caring\", \"wolves\"]\n",
        "originals = words[:10]\n",
        "\n",
        "# Apply stemming to each word\n",
        "stemmed_words = [porter_stemmer.stem(word) for word in originals]\n",
        "\n",
        "# Print the results\n",
        "print(\"Original words:\", originals)\n",
        "print(\"Stemmed words:\", stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "GPMA79G5k5UB",
        "outputId": "c1f84339-425e-49d4-8ab8-21806f5363fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['Tokyo', 'Japanese', 'Tky', 'toko', 'officially', 'the', 'Tokyo', 'Metropolis', 'Tkyto', 'is']\n",
            "Stemmed words: ['tokyo', 'japanes', 'tki', 'toko', 'offici', 'the', 'tokyo', 'metropoli', 'tkyto', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Snowball Stemmer**"
      ],
      "metadata": {
        "id": "BgTrbhOXoccN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Snowball Stemmer, compared to the Porter Stemmer, is multi-lingual as it can handle non-English words. It supports various languages and is based on the ‘Snowball’ programming language, known for efficient processing of small strings.\n",
        "\n",
        "The Snowball stemmer is way more aggressive than Porter Stemmer and is also referred to as Porter2 Stemmer. Because of the improvements added when compared to the Porter Stemmer, the Snowball stemmer is having greater computational speed."
      ],
      "metadata": {
        "id": "GkFs_ynColgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Choose a language for stemming, for example, English\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "# Example words to stem\n",
        "words_to_stem = originals\n",
        "\n",
        "# Apply Snowball Stemmer\n",
        "stemmed_words = [stemmer.stem(word) for word in words_to_stem]\n",
        "\n",
        "# Print the results\n",
        "print(\"Original words:\", words_to_stem)\n",
        "print(\"Stemmed words:\", stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5GQ9vwoJnBJB",
        "outputId": "c0bb55f9-9249-4f5a-cab9-dacec23bf42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['Tokyo', 'Japanese', 'Tky', 'toko', 'officially', 'the', 'Tokyo', 'Metropolis', 'Tkyto', 'is']\n",
            "Stemmed words: ['tokyo', 'japanes', 'tki', 'toko', 'offici', 'the', 'tokyo', 'metropoli', 'tkyto', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Lancaster Stemmer**"
      ],
      "metadata": {
        "id": "Bcu80zzbpGb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Lancaster stemmers are more aggressive and dynamic compared to the other two stemmers. The stemmer is really faster, but the algorithm is really confusing when dealing with small words. The Lancaster stemmers are not as efficient as Snowball Stemmers."
      ],
      "metadata": {
        "id": "k_qIkKqGpPKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "# Create a Lancaster Stemmer instance\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "# Example words to stem\n",
        "words_to_stem = originals\n",
        "\n",
        "# Apply Lancaster Stemmer\n",
        "stemmed_words = [stemmer.stem(word) for word in words_to_stem]\n",
        "\n",
        "# Print the results\n",
        "print(\"Original words:\", words_to_stem)\n",
        "print(\"Stemmed words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MKgjUZ3bosbL",
        "outputId": "59cc3751-e7d4-4f0e-a058-267317ed2764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['Tokyo', 'Japanese', 'Tky', 'toko', 'officially', 'the', 'Tokyo', 'Metropolis', 'Tkyto', 'is']\n",
            "Stemmed words: ['tokyo', 'japanes', 'tky', 'toko', 'off', 'the', 'tokyo', 'metropol', 'tkyto', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stopwords Removal**"
      ],
      "metadata": {
        "id": "yTj1kx3Op2MQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords are frequently occurring words in a language that are frequently omitted from natural language processing (NLP) tasks due to their low significance for deciphering textual meaning. The particular list of stopwords can change based on the language being studied and the context. The following is a broad list of stopword categories:\n",
        "\n",
        "*  Common Stopwords: These are the most frequently occurring words in a language and are often removed during text preprocessing. Examples include “the,” “is,” “in,” “for,” “where,” “when,” “to,” “at,” etc.\n",
        "*  Custom Stopwords: Depending on the specific task or domain, additional words may be considered as stopwords. These could be domain-specific terms that don’t contribute much to the overall meaning. For example, in a medical context, words like “patient” or “treatment” might be considered as custom stopwords.\n",
        "*  Numerical Stopwords: Numbers and numeric characters may be treated as stopwords in certain cases, especially when the analysis is focused on the meaning of the text rather than specific numerical values.\n",
        "*  Single-Character Stopwords: Single characters, such as “a,” “I,” “s,” or “x,” may be considered stopwords, particularly in cases where they don’t convey much meaning on their own.\n",
        "*  Contextual Stopwords: Words that are stopwords in one context but meaningful in another may be considered as contextual stopwords. For instance, the word “will” might be a stopword in the context of general language processing but could be important in predicting future events."
      ],
      "metadata": {
        "id": "TSoXkkOBqClr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "qI_wip6Oo-zn",
        "outputId": "f137f105-8099-4a13-df63-69d7ad8f15d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Removing stop words with NLTK**"
      ],
      "metadata": {
        "id": "yFQJQEubqqeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sent = \"Japanese author Haruki Murakami has based some of his novels in Tokyo including Norwegian Wood and David Mitchells first two novels numberdream and Ghostwritten featured the city.\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "\n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "UA5-eGXdqc_4",
        "outputId": "7f2519fd-3e72-4665-b6b7-c1d7bf85ecc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Japanese', 'author', 'Haruki', 'Murakami', 'has', 'based', 'some', 'of', 'his', 'novels', 'in', 'Tokyo', 'including', 'Norwegian', 'Wood', 'and', 'David', 'Mitchells', 'first', 'two', 'novels', 'numberdream', 'and', 'Ghostwritten', 'featured', 'the', 'city', '.']\n",
            "['Japanese', 'author', 'Haruki', 'Murakami', 'based', 'novels', 'Tokyo', 'including', 'Norwegian', 'Wood', 'David', 'Mitchells', 'first', 'two', 'novels', 'numberdream', 'Ghostwritten', 'featured', 'city', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Removing stop words with SpaCy**"
      ],
      "metadata": {
        "id": "URd0pbwMrW98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"Japanese author Haruki Murakami has based some of his novels in Tokyo including Norwegian Wood and David Mitchells first two novels numberdream and Ghostwritten featured the city.\"\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "# Join the filtered words to form a clean text\n",
        "clean_text = ' '.join(filtered_words)\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Text after Stopword Removal:\", clean_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "gZN3Ts3_qvbS",
        "outputId": "0a5e9d56-9627-4e72-8dce-231dae5ae9bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Japanese author Haruki Murakami has based some of his novels in Tokyo including Norwegian Wood and David Mitchells first two novels numberdream and Ghostwritten featured the city.\n",
            "Text after Stopword Removal: Japanese author Haruki Murakami based novels Tokyo including Norwegian Wood David Mitchells novels numberdream Ghostwritten featured city .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w9LyD_ZNrdR3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}